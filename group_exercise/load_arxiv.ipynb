{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffdcac87",
   "metadata": {},
   "source": [
    "# Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d405595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import sys\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cb2854",
   "metadata": {},
   "source": [
    "Due to GitHub's limit's on file size, we can't have the full data on GH. However, it is \"small\" enough that you could, if you wanted, get it all from Hugging Face. It should be around $5$ GBs of memory or so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96284f13",
   "metadata": {},
   "source": [
    "### Full Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a couple of minutes, depending on where you are working.\n",
    "arxiv_dataset = load_dataset(\"neuralwork/arxiver\", split= 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b07ba7",
   "metadata": {},
   "source": [
    "If you want to check approximate memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_gb = sum([sys.getsizeof(article['markdown']) for article in arxiv_dataset])/1e9\n",
    "print(f\"The size of the text only is: {size_gb} GBs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4668e590",
   "metadata": {},
   "source": [
    "**Random sample from full data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8396fa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "k = 500\n",
    "arxiv_sample = arxiv_dataset.shuffle(seed=seed).select(range(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23986903",
   "metadata": {},
   "source": [
    "### Subsample the data (no shuffling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd16d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 500\n",
    "arxiv_dataset = load_dataset(\"neuralwork/arxiver\", split= 'train', streaming=True)\n",
    "arxiv_sample = list(islice(arxiv_dataset, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc496a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_gb = sum([sys.getsizeof(paper['markdown']) for paper in arxiv_sample])/1e6\n",
    "print(f\"The size of the text only is: {size_gb} MBs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
