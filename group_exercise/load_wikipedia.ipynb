{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4da28e8d",
   "metadata": {},
   "source": [
    "# Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803d7e52",
   "metadata": {},
   "source": [
    "Sample code for loading the wikipedia dataset. Note: the full dataset has about $6.4M$ entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46878aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from itertools import islice\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f983d194",
   "metadata": {},
   "source": [
    "### Option 1 - Loading first K rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4624a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1000\n",
    "streamed_wikipedia = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train\", streaming=True)\n",
    "wiki_sample = list(islice(streamed_wikipedia, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b747d832",
   "metadata": {},
   "source": [
    "If you want to check approximate memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26a0486",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_gb = sum([sys.getsizeof(page['text']) for page in wiki_sample])/1e9\n",
    "print(f\"The size of the text only is: {size_gb} GBs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283d4da4",
   "metadata": {},
   "source": [
    "### Option 2 - Loading random subsample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87fff1b",
   "metadata": {},
   "source": [
    "I learned this trick from chatgpt, and quite like it! It is called \"reservoir sampling\". It takes longer but does not exhaust your memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1035539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reservoir sample algorithm. samples from online streaming (not batch) ensuring at the end each data point has same prob.\n",
    "def reservoir_sample(dataset_stream, k, nmax, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    reservoir = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i < nmax:\n",
    "            if i < k:\n",
    "                reservoir.append(example)\n",
    "            else:\n",
    "                j = random.randint(0, i)\n",
    "                if j < k:\n",
    "                    reservoir[j] = example\n",
    "        else:\n",
    "            print(f\"You've reached the maximum of {nmax}!\")\n",
    "            break\n",
    "    return reservoir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca145bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset but not all in one batch, so streaming, then reservoir sample\n",
    "# k is the size of the actual sample, nmax is how far down the dataset it samples. if you wanted to go over the whole dataset\n",
    "    # you can set it to around 6.4M but I don't know how long it will take. Colab in particular may be significantly slow\n",
    "\n",
    "k = 2500\n",
    "nmax = 10000 #max=10K works well\n",
    "seed = 42\n",
    "streamed_wikipedia = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train\", streaming=True)\n",
    "wiki_sample = reservoir_sample(streamed_wikipedia, k=k, nmax=nmax, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b59e1",
   "metadata": {},
   "source": [
    "If you want to check approximate memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18937fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_gb = sum([sys.getsizeof(page['text']) for page in wiki_sample])/1e9\n",
    "print(f\"The size of the text only is: {size_gb} GBs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb5ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-week-rag (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
