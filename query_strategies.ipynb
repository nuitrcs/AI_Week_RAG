{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9887c013",
   "metadata": {},
   "source": [
    "# Advanced Query Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231409b",
   "metadata": {},
   "source": [
    "<span style=\"text-transform: uppercase;\n",
    "        font-size: 14px;\n",
    "        letter-spacing: 1px;\n",
    "        font-family: 'Segoe UI', sans-serif;\">\n",
    "    Author\n",
    "</span><br>\n",
    "efrén cruz cortés\n",
    "<hr style=\"border: none; height: 1px; background: linear-gradient(to right, transparent 0%, #ccc 10%, transparent 100%); margin-top: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1c464a",
   "metadata": {},
   "source": [
    "Querying is not just about embedding a sentence / document and searching for the closest match. We can get very creative when building querying vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60805248",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7806b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Helper libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba32471",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf34ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model\n",
    "model_name = 'all-mpnet-base-v2' \n",
    "emb_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94ebf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lyrics data and embeddings\n",
    "data_path = \"https://raw.githubusercontent.com/nuitrcs/AI_Week_RAG/refs/heads/main/data/songs.csv\"\n",
    "lyrics = pd.read_csv(data_path)\n",
    "embeddings = emb_model.encode(lyrics['Lyrics'], normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc6ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# faiss index\n",
    "d_emb = len(embeddings[0])\n",
    "faiss_index = faiss.IndexFlatIP(d_emb)\n",
    "faiss_index.add(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87afb11f",
   "metadata": {},
   "source": [
    "## Averaging 2 queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b7af83",
   "metadata": {},
   "source": [
    "You can average two concepts and find the document that lies somewhere in between!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare Swift and Dylan\n",
    "lyrics_swilan = lyrics[lyrics['Artist'].isin(['Taylor Swift', 'Bob Dylan'])].copy()\n",
    "lyrics_swilan = lyrics_swilan.reset_index(drop=True)\n",
    "\n",
    "# new embeddings\n",
    "embeddings_swilan = emb_model.encode(lyrics_swilan['Lyrics'].to_list(), normalize_embeddings=True)\n",
    "\n",
    "# new faiss\n",
    "d_emb = len(embeddings_swilan[0])\n",
    "faiss_swilan = faiss.IndexFlatIP(d_emb)\n",
    "faiss_swilan.add(embeddings_swilan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries\n",
    "q1 = \"a heartbreaking love story\"\n",
    "q2 = \"a long journey on the open road\"\n",
    "\n",
    "# Encode and average\n",
    "v_q1 = emb_model.encode(q1, normalize_embeddings=True)\n",
    "v_q2 = emb_model.encode(q2, normalize_embeddings=True)\n",
    "v_avg = (v_q1 + v_q2) / 2\n",
    "v_avg = v_avg / np.linalg.norm(v_avg)  # re-normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed457d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search extreme songs - love\n",
    "_, I_love = faiss_swilan.search(np.expand_dims(v_q1, axis=0), k=1)\n",
    "print(lyrics_swilan.loc[I_love[0,0], ['Artist', 'Title']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb05169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extreme song - journey\n",
    "_, I_journey = faiss_swilan.search(np.expand_dims(v_q2, axis=0), k=1)\n",
    "print(lyrics_swilan.loc[I_journey[0,0], ['Artist', 'Title']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7814116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search\n",
    "D, I = faiss_swilan.search(np.expand_dims(v_avg, axis=0), k=5)\n",
    "\n",
    "# Show results\n",
    "for idx, score in zip(I[0], D[0]):\n",
    "    print(lyrics_swilan.iloc[idx][\"Artist\"], \"-\", lyrics_swilan.iloc[idx][\"Title\"], \"| score:\", round(score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033e79e2",
   "metadata": {},
   "source": [
    "## Averaging multiple queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b820fb",
   "metadata": {},
   "source": [
    "You can also average more than two queries, hence creating a semantic \"centroid\". This can work as a central theme, an abstraction of different examples, etc.\n",
    "\n",
    "Let's try two different prototypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7aef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "love_queries = [\n",
    "    \"falling in love\",\n",
    "    \"heartbreak after a breakup\",\n",
    "    \"missing someone you loved\",\n",
    "    \"longing for someone far away\",\n",
    "    \"the crushing pain of unrequited love\",\n",
    "    \"healing after heartbreak\",\n",
    "    \"a secret crush\",\n",
    "    \"an emotional love story with ups and downs\",\n",
    "    \"falling in love again!\",\n",
    "    \"getting over you\"\n",
    "]\n",
    "\n",
    "# compute the centroid\n",
    "love_embs = emb_model.encode(love_queries, normalize_embeddings=True)\n",
    "love_prototype = np.mean(love_embs, axis=0)\n",
    "love_prototype = love_prototype / np.linalg.norm(love_prototype)  # re-normalize\n",
    "\n",
    "# search\n",
    "D, I = faiss_swilan.search(np.expand_dims(love_prototype, axis=0), k=5)\n",
    "\n",
    "# and let's check which songs are more prototypical\n",
    "for idx, score in zip(I[0], D[0]):\n",
    "    print(lyrics_swilan.iloc[idx][\"Artist\"], \"-\", lyrics_swilan.iloc[idx][\"Title\"], \"| score:\", round(score, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9025fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_queries = [\n",
    "    \"wandering down the road\",\n",
    "    \"restless and moving on\",\n",
    "    \"searching for meaning in life\",\n",
    "    \"socially conscious\",\n",
    "    \"feeling like an outsider\",\n",
    "    \"stories about change and transformation\",\n",
    "    \"a restless spirit looking for answers\",\n",
    "    \"a voice of social awareness\",\n",
    "    \"poetic reflections on the world\",\n",
    "    \"the open highway calling\"\n",
    "]\n",
    "\n",
    "# compute the centroid\n",
    "rolling_embs = emb_model.encode(rolling_queries, normalize_embeddings=True)\n",
    "rolling_prototype = np.mean(rolling_embs, axis=0)\n",
    "rolling_prototype = rolling_prototype / np.linalg.norm(rolling_prototype)  # re-normalize\n",
    "\n",
    "# search\n",
    "D, I = faiss_swilan.search(np.expand_dims(rolling_prototype, axis=0), k=5)\n",
    "\n",
    "# and let's check which songs are more prototypical\n",
    "for idx, score in zip(I[0], D[0]):\n",
    "    print(lyrics_swilan.iloc[idx][\"Artist\"], \"-\", lyrics_swilan.iloc[idx][\"Title\"], \"| score:\", round(score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20df276",
   "metadata": {},
   "source": [
    "## Semantic walks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed66e973",
   "metadata": {},
   "source": [
    "OK, now let's take a promenade in conceptual space. Let's say we have two queries (or two pre-established songs), and you want to explore what lies in between as we move from one to another. We definitely can!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2054b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start and end queries\n",
    "q_start = \"a heartbreaking love story, missing someone you loved\"\n",
    "q_end   = \"wandering through changing times, a contemplative journey on life and society\"\n",
    "\n",
    "v_start = emb_model.encode(q_start, normalize_embeddings=True)\n",
    "v_end   = emb_model.encode(q_end, normalize_embeddings=True)\n",
    "\n",
    "# Number of interludes along the promenade\n",
    "n_steps = 10\n",
    "\n",
    "# compute the interludes (this is just linear interpolation, in case you're interested)\n",
    "interlude_vectors = []\n",
    "for t in np.linspace(0, 1, n_steps):\n",
    "    v_step = (1-t)*v_start + t*v_end\n",
    "    v_step = v_step / np.linalg.norm(v_step)\n",
    "    interlude_vectors.append(v_step)\n",
    "\n",
    "# find closest songs to each intermediate spot\n",
    "for i, v in enumerate(interlude_vectors):\n",
    "    D, I = faiss_swilan.search(np.expand_dims(v, axis=0), k=3)\n",
    "    print(f\"\\nStep {i+1}\")\n",
    "    for idx, score in zip(I[0], D[0]):\n",
    "        song = lyrics_swilan.iloc[idx]\n",
    "        print(f\"{song['Artist']} - {song['Title']} | score: {round(score,3):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090660e4",
   "metadata": {},
   "source": [
    "Let's now try with our full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15dcfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start and end queries\n",
    "q_start = \"a joyful holiday celebration with friends and family, festive winter songs with warmth and cheer\"\n",
    "q_end   = \"a heartfelt love story with emotional ups and downs, reflecting on personal growth and change\"\n",
    "\n",
    "v_start = emb_model.encode(q_start, normalize_embeddings=True)\n",
    "v_end   = emb_model.encode(q_end, normalize_embeddings=True)\n",
    "\n",
    "# Number of interludes along the promenade\n",
    "n_steps = 10\n",
    "\n",
    "# compute the interludes (this is just linear interpolation, in case you're interested)\n",
    "interlude_vectors = []\n",
    "for t in np.linspace(0, 1, n_steps):\n",
    "    v_step = (1-t)*v_start + t*v_end\n",
    "    v_step = v_step / np.linalg.norm(v_step)\n",
    "    interlude_vectors.append(v_step)\n",
    "\n",
    "# find closest songs to each intermediate spot\n",
    "for i, v in enumerate(interlude_vectors):\n",
    "    D, I = faiss_index.search(np.expand_dims(v, axis=0), k=3)\n",
    "    print(f\"\\nStep {i+1}\")\n",
    "    for idx, score in zip(I[0], D[0]):\n",
    "        song = lyrics.iloc[idx]\n",
    "        print(f\"{song['Artist']} - {song['Title']} | score: {round(score,3):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f4c688",
   "metadata": {},
   "source": [
    "You can also explore what lies between two songs already in your dataset, you don't even need queries to take your semantic promenade :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e147e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-workshop-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
