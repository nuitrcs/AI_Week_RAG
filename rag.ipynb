{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6ab5a09",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3882a9",
   "metadata": {},
   "source": [
    "**What is Retrieval augmented generation?**\n",
    "\n",
    "See slides!\n",
    "\n",
    "**What libraries will we use?**\n",
    "- Embedding step: `sentence_transformers`, other good options available.\n",
    "- Indexing step: `faiss`, it's my favorite so far.\n",
    "- Generation step: `transformers` or `ollama`.\n",
    "\n",
    "**Pre-requisites**\n",
    "- Basic python, including numpy\n",
    "- The intro slides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39ecb99",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1401c5b",
   "metadata": {},
   "source": [
    "TO-DO:\n",
    "* Install faiss-cpu or faiss-gpu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b868b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for better performance when loading xet models:\n",
    "# pip install hf_xet\n",
    "\n",
    "# or:\n",
    "#pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f155021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load olmo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9c943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_google_colab = 1\n",
    "# if in_google_colab:\n",
    "#   !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae7dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dictionary to handle cases where package name and module name are different\n",
    "# package_to_module = {\n",
    "#     \"faiss-cpu\": \"faiss\",\n",
    "#     \"scikit-learn\": \"sklearn\",\n",
    "#     \"umap-learn\": \"umap\"\n",
    "# }\n",
    "\n",
    "# Packages to install\n",
    "# packages = [\"faiss-cpu\", \"numpy\", \"scikit-learn\", \"umap-learn\"]\n",
    "\n",
    "# def check_and_install_packages(package_list):\n",
    "#     \"\"\"Check if each package in the list is installed, and install it if not.\"\"\"\n",
    "#     for package in package_list:\n",
    "#         # Get the correct module name (or fallback to package name if they are the same)\n",
    "#         module_name = package_to_module.get(package, package)\n",
    "#         try:\n",
    "#             __import__(module_name)  # Try to import the correct module name\n",
    "#             print(f\"{package} ({module_name}) is already installed\")\n",
    "#         except ImportError:\n",
    "#             print(f\"{package} is not installed, installing now...\")\n",
    "#             !pip install {package}\n",
    "#             # And add import here no?\n",
    "\n",
    "# # Check and install packages\n",
    "# check_and_install_packages(packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13ea87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helps with threading issues\n",
    "# import os\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5190cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from transformers import pipeline\n",
    "import ollama\n",
    "\n",
    "# Machine learning libraries\n",
    "from umap import UMAP\n",
    "\n",
    "# Helper libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualization libraries\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6144f9d0",
   "metadata": {},
   "source": [
    "## Section 1: Building the Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f238944",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0beed",
   "metadata": {},
   "source": [
    "We first need \"anchoring\" data that our system will retrieve as necessary to answer to relevant prompts. In our case, we are going to use a dataset of song lyrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220aae5",
   "metadata": {},
   "source": [
    "Indicate the data directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f8979",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data/songs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d549f580",
   "metadata": {},
   "source": [
    "Load the data into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af3d058",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c073e5",
   "metadata": {},
   "source": [
    "Let's take a look at the first few songs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d507015",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7d62a",
   "metadata": {},
   "source": [
    "Check how many songs we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b794287",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a79d551",
   "metadata": {},
   "source": [
    "And let's look at the artists we have songs from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f791a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics['Artist'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9583c45",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371fa456",
   "metadata": {},
   "source": [
    "There are many elements of preprocessing. The main ones you will encounter are:\n",
    "* Removing boilerplate text, cleaning white spaces, etc.\n",
    "* Tokenization\n",
    "* Chunking\n",
    "\n",
    "I have done the first one for you. In this workshop we may not have time to go over tokenization or chunking, but I've written sections about them at the end of the notebook, in the *Optional* section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef6654",
   "metadata": {},
   "source": [
    "### Create Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5ae6c",
   "metadata": {},
   "source": [
    "This will be our first crucial step. To create embeddings, you need to have a model that is designed for the same type of data as your data! There are models for text, for images, multimodal, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560e66bd",
   "metadata": {},
   "source": [
    "**Step 1** Choose your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33c4fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pick a popular text model\n",
    "model_name = 'all-mpnet-base-v2' \n",
    "\n",
    "# We create the SentenceTransformer based on our model. This is the function that takes texts and produces embeddings.\n",
    "emb_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891d3c6f",
   "metadata": {},
   "source": [
    "**Step 2** Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bda5030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just one line!!\n",
    "embeddings = emb_model.encode(lyrics['Lyrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ad715",
   "metadata": {},
   "source": [
    "Now let's take a second to study our embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2561d4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentenceTransformer returns numpy arrays. Other libraries may return different data types.\n",
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c180ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The numpy array is basically an nxd matrix, where n is number of songs and d is the embedding dimension\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f587d635",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**<span style=\"color:red\">EXERCISE</span>** <span style=\"color:darkred\"><<*ERASE solutions after debugging*>></span>\n",
    "\n",
    "Different embedding models will perform differently. Indeed, some are trained for specific purpuses in mind (Q&A, semantic search, multimodality, etc.). Throughout the exercises you'll be comparing our running example model with a diffrent embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8c36ce",
   "metadata": {},
   "source": [
    "1. Create an embedding SentenceTransformer using the model `all-MiniLM-L6-v2`. For comparison, this model is smaller than our running example model (80MB, compared to 500MB) Call it something different to what we have above, for example `exercise_emb_model`.\n",
    "\n",
    "2. Create embeddings of our lyrics for your exercise model. Call them something like `exercise_lyrics`.\n",
    "\n",
    "3. Check if the number of dimensions is the same for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SentenceTransformer here\n",
    "exercise_emb_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62622d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings here\n",
    "exercise_emebeddings = exercise_emb_model.encode(lyrics['Lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "165d60b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(745, 384)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of dimensions\n",
    "exercise_emebeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a30988",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dff68b",
   "metadata": {},
   "source": [
    "**Normalization**\n",
    "\n",
    "Note that our vectors are normalized (euclidean norm is 1). This is not always the case, but it is very important you know if your vectors are normalized or not. We'll get back to it when we create our index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3dcc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.inner(embeddings[0], embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec758b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization is not always ensured by default, but you can set it to be so with an argument:\n",
    "embeddings = emb_model.encode(lyrics['Lyrics'], normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a02e2d",
   "metadata": {},
   "source": [
    "SentenceTransformers provides a handy `similarity` function, which computes the pairwise similarity of two sets of songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb0a95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing two songs\n",
    "emb_model.similarity(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7f66f0",
   "metadata": {},
   "source": [
    "The result is a *tensor*, which you can index as you would with numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d01 = emb_model.similarity(embeddings[0], embeddings[1])\n",
    "print(f\"The cosine similarity between song 0 and song 1 is {d01[0,0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf8d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A maximum of 1 is achieved if the vectors are the same\n",
    "emb_model.similarity(embeddings[0], embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529360aa",
   "metadata": {},
   "source": [
    "You can compare one song to multiples songs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ff5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model.similarity(embeddings[0], embeddings[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08386999",
   "metadata": {},
   "source": [
    "Or multiple songs to multiple songs, in which case you get a matrix of similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ab8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model.similarity(embeddings[0:5], embeddings[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b84d3a",
   "metadata": {},
   "source": [
    "By default, sentence_transformers uses the `cosine` similarity (see slides). But you can use other distances like the euclidean or manhattan distances (see their documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9cf7b9",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**<span style=\"color:red\">EXERCISE</span>** <span style=\"color:darkred\"><<*ERASE solutions after debugging*>></span>\n",
    "\n",
    "1. Check if the Euclidean norm of some of your `exercise_embeddings` is 1 or not.\n",
    "2. Calculate the cosine similarity for the first 5 of your exercise embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4bb1bb94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(1.0)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.inner(exercise_emebeddings[0], exercise_emebeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9527eceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.3810, 0.5573, 0.6025, 0.5036],\n",
       "        [0.3810, 1.0000, 0.4271, 0.4929, 0.4268],\n",
       "        [0.5573, 0.4271, 1.0000, 0.5127, 0.3940],\n",
       "        [0.6025, 0.4929, 0.5127, 1.0000, 0.4785],\n",
       "        [0.5036, 0.4268, 0.3940, 0.4785, 1.0000]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exercise_emb_model.similarity(exercise_emebeddings[0:5], exercise_emebeddings[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36e909",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f1f965",
   "metadata": {},
   "source": [
    "### Visualization for Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b44e354",
   "metadata": {},
   "source": [
    "**Similarity Heatmap**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2afa4d7",
   "metadata": {},
   "source": [
    "Before continuing, let's try to develop an intuition about these embeddings.\n",
    "\n",
    "First, recall emebeddings are vectors in a d-dimensional space, where d is quite large. We can't visualize them directly, but we can see how they interact with each other. Let's look at a heatmap of the similarities between different artists songs.\n",
    "\n",
    "All the code in this section will be skipped! It is not important for our workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1cc3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heatmap = 5\n",
    "a_artist = 'Taylor Swift'\n",
    "b_artist = 'Bob Dylan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de3a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics[lyrics['Artist']==a_artist].head(n_heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e52659",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics[lyrics['Artist']==b_artist].head(n_heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a188b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the indices for easy access\n",
    "a_idxs = lyrics[lyrics['Artist']==a_artist].index.to_list()[:n_heatmap]\n",
    "b_idxs = lyrics[lyrics['Artist']==b_artist].index.to_list()[:n_heatmap]\n",
    "\n",
    "# subset embeddings of first and last n songs\n",
    "a_embs = embeddings[a_idxs]\n",
    "b_embs = embeddings[b_idxs]\n",
    "both_embs = np.concatenate((a_embs, b_embs), axis=0)\n",
    "\n",
    "# we'll use this in our visualization:\n",
    "a_titles = lyrics['Title'].iloc[:n_heatmap].to_list()\n",
    "b_titles = lyrics['Title'].iloc[-n_heatmap:].to_list()\n",
    "a_titles = [title[:20] for title in a_titles] # truncating text\n",
    "b_titles = [title[:20] for title in b_titles]\n",
    "both_titles = a_titles + b_titles\n",
    "\n",
    "# compute their similarity, we want to visualize this with a heatmap\n",
    "fl_sim_matrix = emb_model.similarity(both_embs, both_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1917d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(\n",
    "    fl_sim_matrix,\n",
    "    x=both_titles,\n",
    "    y=both_titles,\n",
    "    color_continuous_scale=\"Viridis\",\n",
    "    text_auto=\".2f\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Cosine Similarity Among {a_artist} and {b_artist} Lyrics\",\n",
    "    width=750,\n",
    "    height=750,\n",
    "    xaxis=dict(tickangle=45)\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5575f379",
   "metadata": {},
   "source": [
    "**Dimensionality Reduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e185340d",
   "metadata": {},
   "source": [
    "We can also visualize embeddings by looking at them in a lower dimension. We will use a machine learning technique called *dimensionality reduction*. You don't need to know how it's done, and don't worry about the code, we'll use it only for visualization purposes.\n",
    "\n",
    "(If you attend the topic modeling workshop, you may learn about it more in depth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3cb27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimred_model = UMAP(\n",
    "    n_neighbors=3,  # umap hyper-parameter\n",
    "    n_components=2, # dimension we are reducing to\n",
    "    metric='cosine'\n",
    ")\n",
    "\n",
    "two_d_rep = dimred_model.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa96d85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_clustering = go.Figure()\n",
    "\n",
    "fig_clustering.add_trace(go.Scatter(\n",
    "    x=two_d_rep[:, 0],\n",
    "    y=two_d_rep[:,1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=6),\n",
    "    text=lyrics['Title'],\n",
    "    hoverinfo='text'\n",
    "))\n",
    "\n",
    "fig_clustering.update_layout(\n",
    "    height=750, width=750,\n",
    "    title='Low dimensional view of embedded lyrics',\n",
    ")\n",
    "\n",
    "fig_clustering.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa77e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A reminder of the artists, which one would you like to see?\n",
    "lyrics['Artist'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31e9548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's highlight an artist's songs just for fun:\n",
    "artist_highlight = 'John Denver'\n",
    "artist_idxs = lyrics[lyrics['Artist']==artist_highlight].index.to_list()\n",
    "\n",
    "fig_clustering = go.Figure()\n",
    "\n",
    "fig_clustering.add_trace(go.Scatter(\n",
    "    x=two_d_rep[:, 0],\n",
    "    y=two_d_rep[:,1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=6),\n",
    "    text=lyrics['Title'],\n",
    "    hoverinfo='text',\n",
    "    name = 'All artists'\n",
    "))\n",
    "\n",
    "fig_clustering.add_trace(go.Scatter(\n",
    "    x=two_d_rep[artist_idxs, 0],\n",
    "    y=two_d_rep[artist_idxs,1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=6, color='crimson'),\n",
    "    text = lyrics['Title'].iloc[artist_idxs],\n",
    "    hoverinfo = 'text',\n",
    "    name = artist_highlight\n",
    "))\n",
    "\n",
    "fig_clustering.update_layout(\n",
    "    height=750, width=750,\n",
    "    title='Low dimensional view of embedded lyrics',\n",
    ")\n",
    "\n",
    "fig_clustering.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a9ec79",
   "metadata": {},
   "source": [
    "### Create Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f36a19f",
   "metadata": {},
   "source": [
    "OK, back to our main task. Now we have this collection of high-dimensional embeddings. It's time to create our index! As a reminder, an index is a data structure which efficiently allows us to find the most similar vectors in our collection to one reference point (usually, a user's query). See slides if you need a refresher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b130eb",
   "metadata": {},
   "source": [
    "Creating a fass index takes one line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad13b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the dimension of our embeddings\n",
    "d_emb = len(embeddings[0])\n",
    "\n",
    "# Create a faiss index\n",
    "faiss_index = faiss.IndexFlatIP(d_emb) # <-- creating the d-dimensional index (empty for now)\n",
    "print(faiss_index.is_trained)\n",
    "print(faiss_index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5ffe09",
   "metadata": {},
   "source": [
    "We will talk about the meaning of `FaltIP` later. For now, just know that **only because our embeddings are normalized** this index works with the cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce28e2e",
   "metadata": {},
   "source": [
    "### Add embeddings to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2043ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd68b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(faiss_index.is_trained)\n",
    "print(faiss_index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7540a62",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**<span style=\"color:red\">EXERCISE</span>** <span style=\"color:darkred\"><<*ERASE solutions after debugging*>></span>\n",
    "\n",
    "1. Create a new `exercise_index` and add your exercise emebeddings to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc82497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c764916",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4141e39",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40920151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a query and embed it, don't forget to normalize!\n",
    "query_1 = \"Life is good and I will survive. I am happy that things turned out this way\"\n",
    "qemb_1 = emb_model.encode(query_1, normalize_embeddings=True)\n",
    "\n",
    "# Let's remind ourselves how the embedding is returned:\n",
    "print(f\"Query embedding info:\\n\\nType: {type(qemb_1)}\\nShape: {qemb_1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08177b",
   "metadata": {},
   "source": [
    "In theory, we can ask `faiss` to find the closest songs to it with just one line of code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9a6959",
   "metadata": {},
   "source": [
    "```python\n",
    "faiss_index.search(qemb_1, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e3db20",
   "metadata": {},
   "source": [
    "However, the above will produce an error because `faiss` expects a (n,d) numpy array, but when we encode only one query we get a (d,) numpy array back. Therefore, we need to reshape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d9098",
   "metadata": {},
   "outputs": [],
   "source": [
    "qemb_1 = qemb_1.reshape((1, qemb_1.shape[0])) # reshape into (1,d)\n",
    "\n",
    "print(f\"Query embedding info:\\n\\nType: {type(qemb_1)}\\nShape: {qemb_1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cb6bee",
   "metadata": {},
   "source": [
    "Now we can search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2100ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index.search(qemb_1, 1) # the 1 indicates how many close neighbors to find."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c087539f",
   "metadata": {},
   "source": [
    "The first element [[.43]] is the cosine similarity between the first song and its closest neighbor. The second element [[294]] is the index (song number) of such neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc1e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_sim, neighbor_idx = faiss_index.search(qemb_1, 1)\n",
    "\n",
    "print(f\"The cosine similarity to the closest neighbor is: {neighbor_sim[0,0]:.2f}\\n\")\n",
    "print(f\"And the neighbor is:\\n{lyrics.iloc[294]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e5a5a4",
   "metadata": {},
   "source": [
    "Pay special attention to the indexing of the results. If you see double brackets [[]] think of it as a matrix, so access it either as [i][j] or [i,j]. Even if we get one result, we'll be given, as output, a 1x1 matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bc9c43",
   "metadata": {},
   "source": [
    "If we wanted to find the k closest neighors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cae602",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "faiss_index.search(qemb_1, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca178193",
   "metadata": {},
   "source": [
    "### Searching multiple queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b21d0",
   "metadata": {},
   "source": [
    "You can search for multiple queries at the same time, however, some `faiss` versions are buggy when that happens. To avoid any complications, we'll just have to search individually for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a86d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a helper function that automatically normalized and reshapes embeddings for us:\n",
    "def embed_reshape(query):\n",
    "    qemb = emb_model.encode(query, normalize_embeddings=True)\n",
    "    qemb = qemb.reshape((1, qemb.shape[0]))\n",
    "    return qemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6825088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"Life is good and I will survive. I am happy that things turned out this way\"\n",
    "query_2 = \"Why did you leave me? I am so sad. The world is so cruel.\"\n",
    "queries = [query_1, query_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208de1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the queries, remember to normalize\n",
    "qembs = [embed_reshape(q) for q in queries]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba313d4",
   "metadata": {},
   "source": [
    "Let's find the 4 closest songs to each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0201bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "D_matched = []\n",
    "I_matched = []\n",
    "for qe in qembs:\n",
    "    dists_q_matched, idxs_q_matched = faiss_index.search(qe, k)\n",
    "    D_matched.append(dists_q_matched)\n",
    "    I_matched.append(idxs_q_matched)\n",
    "# distances_q_matched, indices_q_matched = faiss_index.search(qembs, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b4bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee49604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "I_matched[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df30abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at matched songs for the first query:\n",
    "print(f\"Matched song to your query \\'{query_1}\\':\\n\")\n",
    "for i in I_matched[0][0]:\n",
    "    artist = lyrics['Artist'].iloc[i]\n",
    "    title = lyrics['Title'].iloc[i]\n",
    "    song_lyrics = lyrics['Lyrics'].iloc[i]\n",
    "    print(f\"Artist: {artist}\\nTitle: {title}\\nLyrics:{song_lyrics[:100]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1557c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at matched songs for the first query:\n",
    "print(f\"Matched song to your query \\'{query_2}\\':\\n\")\n",
    "for i in I_matched[1][0]:\n",
    "    artist = lyrics['Artist'].iloc[i]\n",
    "    title = lyrics['Title'].iloc[i]\n",
    "    song_lyrics = lyrics['Lyrics'].iloc[i]\n",
    "    print(f\"Artist: {artist}\\nTitle: {title}\\nLyrics:{song_lyrics[:100]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8816a497",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**<span style=\"color:red\">EXERCISE</span>** <span style=\"color:darkred\"><<*ERASE solutions after debugging*>></span>\n",
    "\n",
    "1. Search for the 4 closest songs to the first query using your exercise index.\n",
    "2. Compare the results to what we got in our main index. Are the songs the same? Do they actually make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc118ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "285ecd3f",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284eaee",
   "metadata": {},
   "source": [
    "### RECAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a2ec7",
   "metadata": {},
   "source": [
    "OK, let's do a quick recap, now that we have developed an intuition, we can actually perform the above steps super quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d711599e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.42986214, 0.41438007]], dtype=float32), array([[294, 608]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# :: Load the data ::\n",
    "data_path = Path(\"data/songs.csv\")\n",
    "lyrics = pd.read_csv(data_path)\n",
    "\n",
    "# :: Create embeddings ::\n",
    "model_name = 'all-mpnet-base-v2' \n",
    "emb_model = SentenceTransformer(model_name)\n",
    "embeddings = emb_model.encode(lyrics['Lyrics'], normalize_embeddings=True)\n",
    "\n",
    "# :: Create index ::\n",
    "d_emb = len(embeddings[0])\n",
    "faiss_index = faiss.IndexFlatIP(d_emb)\n",
    "# Add embeddings to index\n",
    "faiss_index.add(embeddings)\n",
    "\n",
    "# :: Search ::\n",
    "# Make query and embed it\n",
    "query_1 = \"Life is good and I will survive. I am happy that things turned out this way\"\n",
    "qemb_1 = emb_model.encode(query_1, normalize_embeddings=True)\n",
    "# Reshape if necessary\n",
    "qemb_1 = qemb_1.reshape((1, qemb_1.shape[0]))\n",
    "# Search\n",
    "k = 2\n",
    "faiss_index.search(qemb_1, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0a092",
   "metadata": {},
   "source": [
    "And there you have it!! A full retrieval system in less than 20 lines :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc69b13b",
   "metadata": {},
   "source": [
    "### (Optional) A note about indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0729ade",
   "metadata": {},
   "source": [
    "**IndexFlatIP vs IndexFlatL2**\n",
    "\n",
    "IP stands for *inner product*, while L2 stands for *L2 norm* (euclidean distance). In general, these may produce different results, so you need to choose carefully. A simple heuristic is:\n",
    "- Text data: `cosine similarity`.\n",
    "- Image data: `euclidean distance`.\n",
    "\n",
    "But, you should still do some research on the model you are using, the data type, and what you care about (direction, magnitude, etc.). More details on the slides.\n",
    "\n",
    "**Cosine Similarity, Inner Product, Normalization**\n",
    "\n",
    "If and only if your vectors are *normalized*, the cosine similarity is the same as the inner product. In our case, since our embeddings are normalized, we can use `IndexFlatIP` and it will be equivalent to using the cosine similarity, which is what we want.\n",
    "\n",
    "**Flat vs Other Indexes**\n",
    "\n",
    "Our flat index is not the most computationally efficient. It doesn't quantize vectors (see slides) and all searches are brute force (that is, it will compare a query to all vectors in the index). For our toy dataset this is fine, but for larger datasets you should use other indices. Different libraries have implementations of different indices, for example `faiss` has the hierarchical navigable small world index `IndexHNSWFlat`, which is more search efficient, but returns approximate results.\n",
    "\n",
    "We can't go over all index types in this workshop, but here's a handy table for the ones `faiss` offers:\n",
    "https://github.com/facebookresearch/faiss/wiki/Faiss-indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87071f72",
   "metadata": {},
   "source": [
    "### (Optional) Save index on disc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f497f75",
   "metadata": {},
   "source": [
    "In general you will create a large index, and store it on disk for further use. This could be purely for convenience (you don't need to create a new index every time you use your RAG system) but will be vital if your index is quite large and memory becomes a limitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1dbed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to a .index file\n",
    "index_directory = 'rag_workshop.index'\n",
    "faiss.write_index(faiss_index, index_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22acdde",
   "metadata": {},
   "source": [
    "If you need to read it later on, you can use `faiss.read_index(index_directory, faiss.IO_FLAG_MMAP)`, the MMAP flag tells it not to load the full index into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae448314",
   "metadata": {},
   "source": [
    "## Section 2 -  Adding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72ffb04",
   "metadata": {},
   "source": [
    "OK, we are done with the R of RAG. Time to get the A done!\n",
    "\n",
    "For this, we need a generation model and a library that helps us run such model.\n",
    "\n",
    "I recommend either using `transformers` or `ollama`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c03ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The transformers way, if using a gated model like gemma, you'll need to provide an access token\n",
    "# generation_model = \"allenai/OLMo-2-0425-1B\"\n",
    "# gen_ppln = pipeline(task=\"text-generation\", model=generation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c8ca557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason why the sky appears blue rather than pink has to do with a combination of physics, optics, and atmospheric conditions.\n",
      "\n",
      "Here's what happens:\n",
      "\n",
      "1. **Light from the sun**: When sunlight enters Earth's atmosphere, it consists of a broad spectrum of colors, including all the colors of the visible light range (red, orange, yellow, green, blue, indigo, and violet).\n",
      "2. **Scattering of light**: As sunlight travels through the atmosphere, shorter (blue) wavelengths are scattered more than longer (red) wavelengths by the tiny molecules of gases such as nitrogen and oxygen.\n",
      "3. **Rayleigh scattering**: The scattering effect is known as Rayleigh scattering, named after the British physicist Lord Rayleigh, who first described it in the late 19th century. This type of scattering occurs when light interacts with small particles or molecules that are much smaller than the wavelength of light.\n",
      "4. **Blue color dominance**: Since blue light has a shorter wavelength, it is scattered more extensively by the atmosphere's tiny molecules. As a result, our eyes receive more blue light from the sky than any other color.\n",
      "\n",
      "Now, you might be wondering why the sky doesn't appear pink or red, as those colors are also present in sunlight. There are a few reasons for this:\n",
      "\n",
      "* **Pink is scattered less**: Pink light has a longer wavelength than blue light, which means it is scattered less by the atmosphere's tiny molecules.\n",
      "* **Red light passes through**: Longer wavelengths of light, like red and orange, can travel through the atmosphere with fewer scatterings. This is why we see more of these colors in the sky at sunset or sunrise, when the sun's rays pass through a greater amount of atmospheric particles.\n",
      "* **Atmospheric conditions**: The color of the sky can also be influenced by pollution, dust, water vapor, and other atmospheric particles. These factors can scatter light differently, which may change the apparent color of the sky.\n",
      "\n",
      "In summary, the blue color of the sky is a result of the scattering of sunlight by tiny molecules in the atmosphere, with blue light being scattered more extensively than other colors. This is why we see the sky as blue rather than pink or red!\n"
     ]
    }
   ],
   "source": [
    "generation_model = \"llama3.2\"\n",
    "\n",
    "# The ollama way:\n",
    "test_reponse = ollama.generate(\n",
    "    model=generation_model,\n",
    "    prompt=\"Why is the sky blue and not pink?\"\n",
    ")\n",
    "\n",
    "print(test_reponse.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff92eb4",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**<span style=\"color:red\">EXERCISE</span>** <span style=\"color:darkred\"><<*ERASE solutions after debugging*>></span>\n",
    "\n",
    "1. Play a little bit with your generation model. Feel free to ask either serious or silly questions. Provide specific instructions on how you want it to behave, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc8399d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bde8981",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07579b45",
   "metadata": {},
   "source": [
    "### Creating a system prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c674a7",
   "metadata": {},
   "source": [
    "Let's create a system prompt with instructions for the LLM, this system prompt will accept some extra grounding data which we obtain by searching our index. **This is the heart of RAG**, the prompt for our generation model will be supplemented by query-relevant information from a large pool, this is done through the index search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9771ba9",
   "metadata": {},
   "source": [
    "We could write our system prompt here, but these can get quite long, and maybe used in different scripts, hence I recommend writing your prompts in text files and then just loading them. Let's head to `system_prompts/rag_system_prompt.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccdcda9",
   "metadata": {},
   "source": [
    "**(Optional) Quick Review of python Strings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9a8c4e",
   "metadata": {},
   "source": [
    "We'll be modifying the system prompt with the variable output from our LLM, so here's a quick review of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f599b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_q = \"What is the answer to the ultimate question of life, the universe, and everything?\"\n",
    "temp_a = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ecbeaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the answer to the ultimate question of life, the universe, and everything?\n",
      "Answer: 42\n"
     ]
    }
   ],
   "source": [
    "# f-strings\n",
    "f_string = f\"Question: {temp_q}\\nAnswer: {temp_a}\"\n",
    "print(f_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7105cf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the answer to the ultimate question of life, the universe, and everything?\n",
      "Answer: 42\n"
     ]
    }
   ],
   "source": [
    "# .format() with format fields {}\n",
    "temp_text = \"Question: {question}\\nAnswer: {answer}\"\n",
    "temp_text = temp_text.format(question=temp_q, answer=temp_a)\n",
    "print(temp_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf6c2e4",
   "metadata": {},
   "source": [
    "**Load the system prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "81e52198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a snarky art critic with extensive music knowledge. Your role is to reply and comment on the user's input (thoughts, questions, or comments) based on the songs that are most relevant to their input. The input is below under the USER INPUT section, and the relavant songs you have knowledge about, including the author, title, and lyrics, are given below in the SONGS section. Make sure you ground your answers as much as possible on the songs provided.\n",
      "\n",
      "# USER INPUT\n",
      "\n",
      "{user_input}\n",
      "\n",
      "# SONGS\n",
      "\n",
      "{songs}\n"
     ]
    }
   ],
   "source": [
    "sysm_dir = Path('system_prompts/rag_system_prompt.txt')\n",
    "sysm_text = sysm_dir.read_text()\n",
    "print(sysm_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb6de5a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**<span style=\"color:red\">EXERCISE</span>** <span style=\"color:darkred\"><<*ERASE solutions after debugging*>></span>\n",
    "\n",
    "1. Create a new text file called `exercise_sysm.txt` or something like that. Write another system prompt with any instructions your heart desires. Incorporate the as fields both the user input (which can be generic, as in my case, or can be a question, a thought, a chat exchange, etc.).\n",
    "2. If you are using Colab, you will need to upload the file, click on the folder icon on the left menu bar to do so.\n",
    "3. Load the exercise system prompt (call it `exercise_sysm`) and print it to make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14babc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be3598e9",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b19d3e",
   "metadata": {},
   "source": [
    "**Format the System Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "069df019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a snarky art critic with extensive music knowledge. Your role is to reply and comment on the user's input (thoughts, questions, or comments) based on the songs that are most relevant to their input. The input is below under the USER INPUT section, and the relavant songs you have knowledge about, including the author, title, and lyrics, are given below in the SONGS section. Make sure you ground your answers as much as possible on the songs provided.\n",
      "\n",
      "# USER INPUT\n",
      "\n",
      "What is the most philosophical song ever?\n",
      "\n",
      "# SONGS\n",
      "\n",
      "Hey Macarena, ay!\n"
     ]
    }
   ],
   "source": [
    "test_song = 'Hey Macarena, ay!'\n",
    "test_question = 'What is the most philosophical song ever?'\n",
    "full_prompt = sysm_text.format(songs=test_song, user_input=test_question)\n",
    "print(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad3b98a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**<span style=\"color:red\">EXERCISE</span>** <span style=\"color:darkred\"><<*ERASE solutions after debugging*>></span>\n",
    "\n",
    "1. Try the above with your exercise prompt, feel free to input other text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1474654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64306a78",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f5a77",
   "metadata": {},
   "source": [
    "**Generate an answer based on the full prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "47b1899f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Sigh* Oh boy, this is gonna be a challenge. I mean, what's more philosophical than the existential crisis that comes with being forced to dance the Macarena at a wedding reception? \"We're gonna make some noise now...\" Yeah, because that's exactly what life is all about - conforming to societal norms and suppressing individuality in favor of catchy pop hooks.\n",
      "\n",
      "You know what's actually more philosophical, though? \"The Sound of Silence\" by Simon & Garfunkel. The lyrics speak directly to the human condition, with phrases like \"Hello darkness, my old friend / I've come to talk with you again\" echoing the existential crises that we all face at some point in our lives.\n",
      "\n",
      "Or maybe it's \"Stairway to Heaven\" by Led Zeppelin? Robert Plant's wistful lyrics (\"There's a lady who's sure all that glitters is gold\") tap into the human desire for meaning and connection in a seemingly indifferent world. But hey, if you want to talk about the philosophical implications of dancing the Macarena, be my guest.\n"
     ]
    }
   ],
   "source": [
    "test_reponse = ollama.generate(\n",
    "    model=generation_model,\n",
    "    prompt=full_prompt\n",
    ")\n",
    "print(test_reponse.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77af877d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**<span style=\"color:red\">EXERCISE</span>** <span style=\"color:darkred\"><<*ERASE solutions after debugging*>></span>\n",
    "\n",
    "1. Repeat, with your exercise system message + prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79f856d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66f5dc42",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c47a4a9",
   "metadata": {},
   "source": [
    "### Connecting retrieval and generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b1c119",
   "metadata": {},
   "source": [
    "OK, we have all the elements now. Let's put it all together. Let's perform our first RAAAAG!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd685b7",
   "metadata": {},
   "source": [
    "**Part 1 - You create an index from a large database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0cfdbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Load the data ::\n",
    "data_path = Path(\"data/songs.csv\")\n",
    "lyrics = pd.read_csv(data_path)\n",
    "\n",
    "# :: Create embeddings ::\n",
    "model_name = 'all-mpnet-base-v2' \n",
    "emb_model = SentenceTransformer(model_name)\n",
    "embeddings = emb_model.encode(lyrics['Lyrics'], normalize_embeddings=True)\n",
    "\n",
    "# :: Create index ::\n",
    "d_emb = len(embeddings[0])\n",
    "faiss_index = faiss.IndexFlatIP(d_emb)\n",
    "# Add embeddings to index\n",
    "faiss_index.add(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79c93e8",
   "metadata": {},
   "source": [
    "**Step 2 - You perform a query and find the most relevant vectors in your index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ca5aa4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Search ::\n",
    "# Make query and embed it\n",
    "user_query = \"Throughout the echoes of history, nobody has thought as originally as me: I will make a song about August! About the memories and moments we shared in August, before it got away from us. Surely there are no other songs about August, right?\"\n",
    "query_emb = emb_model.encode(user_query, normalize_embeddings=True)\n",
    "# Reshape if necessary\n",
    "query_emb = query_emb.reshape((1, query_emb.shape[0]))\n",
    "# Search\n",
    "k = 2\n",
    "D_matched, I_matched = faiss_index.search(query_emb, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f8ef8a",
   "metadata": {},
   "source": [
    "**Step 3a - You format the relevant songs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f5c56e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_data = []\n",
    "for i, song_idx in enumerate(I_matched[0]):\n",
    "    row = lyrics.iloc[song_idx]\n",
    "    song_data = f\"Song {i}\\nTitle: {row['Title']}\\nAuthor: {row['Artist']}\\nLyrics: {row['Lyrics']}\"\n",
    "    relevant_data.append(song_data)\n",
    "relevant_data = \"\\n\\n\".join(relevant_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dea8a87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song 0\n",
      "Title: august\n",
      "Author: Taylor Swift\n",
      "Lyrics: Salt air, and the rust on your door\n",
      "I never needed anything more\n",
      "Whispers of \"Are you sure?\"\n",
      "\"Never have I ever before\"\n",
      "\n",
      "But I can see us lost in the memory\n",
      "August slipped away into a moment in time\n",
      "'Cause it was never mine\n",
      "And I can see us twisted in bedsheets\n",
      "August sipped away like a bottle of wine\n",
      "'Cause you were never mine\n",
      "\n",
      "Your back beneath the sun\n",
      "Wishin' I could write my name on it\n",
      "Will you call when you're back at school?\n",
      "I remember thinkin' I had you\n",
      "\n",
      "But I can see us lost in the memory\n",
      "August slipped away into a moment in time\n",
      "'Cause it was never mine\n",
      "And I can see us twisted in bedsheets\n",
      "August sipped away like a bottle of wine\n",
      "'Cause you were never mine\n",
      "Back when we were still changin' for the better\n",
      "Wanting was enough\n",
      "For me, it was enough\n",
      "To live for the hope of it all\n",
      "Cancel plans just in case you'd call\n",
      "And say, \"Meet me behind the mall\"\n",
      "So much for summer love and saying \"us\"\n",
      "'Cause you weren't mine to lose\n",
      "You weren't mine to lose, no\n",
      "\n",
      "But I can see us lost in the memory\n",
      "August slipped away into a moment in time\n",
      "'Cause it was never mine\n",
      "And I can see us twisted in bedsheets\n",
      "August sipped away like a bottle of wine\n",
      "'Cause you were never mine\n",
      "\n",
      "'Cause you were never mine\n",
      "Never mine\n",
      "But do you remember?\n",
      "Remember when I pulled up and said \"Get in the car\"\n",
      "And then canceled my plans just in case you'd call?\n",
      "Back when I was livin' for the hope of it all, for the hope of it all\n",
      "\"Meet me behind the mall\"\n",
      "(Remember when I pulled up and said \"Get in the car\")\n",
      "(And then canceled my plans just in case you'd call?)\n",
      "(Back when I was livin' for the hope of it all, for the hope of it all)\n",
      "(\"Meet me behind the mall\")\n",
      "Remember when I pulled up and said \"Get in the car\"\n",
      "And then canceled my plans just in case you'd call?\n",
      "Back when I was livin' for the hope of it all (For the hope of it all)\n",
      "For the hope of it all, for the hope of it all\n",
      "(For the hope of it all, for the hope of it all)\n",
      "\n",
      "Song 1\n",
      "Title: Those Lazy, Hazy, Crazy Days of Summer\n",
      "Author: Nat King Cole\n",
      "Lyrics: Roll out those lazy, hazy, crazy days of summer\n",
      "Those days of soda and pretzels and beer\n",
      "Roll out those lazy, hazy, crazy days of summer\n",
      "Dust off the sun and moon and sing a song of cheer\n",
      "Just fill your basket full of sandwiches and weenies\n",
      "Then lock the house up, now you're set\n",
      "And on the beach you'll see the girls in their bikinis\n",
      "As cute as ever but they never get 'em wet\n",
      "Roll out those lazy, hazy, crazy days of summer\n",
      "Those days of soda and pretzels and beer\n",
      "Roll out those lazy, hazy, crazy days of summer\n",
      "You'll wish that summer could always be here\n",
      "Roll out those lazy, hazy, crazy days of summer\n",
      "Those days of soda and pretzels and beer\n",
      "Roll out those lazy, hazy, crazy days of summer\n",
      "Dust off the sun and moon and sing a song of cheer\n",
      "Don't hafta tell a girl and fella about a drive-in\n",
      "Or some romantic moon it seems\n",
      "Right from the moment that those lovers start arrivin'\n",
      "You'll see more kissin' (whistling) in the cars than on the screen\n",
      "Roll out those lazy, hazy, crazy days of summer\n",
      "Those days of soda and pretzels and beer\n",
      "Roll out those lazy, hazy, crazy days of summer\n",
      "You'll wish that summer could always be here\n",
      "You'll wish that summer could always be here\n",
      "You'll wish that summer could always be here\n"
     ]
    }
   ],
   "source": [
    "print(relevant_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c75645",
   "metadata": {},
   "source": [
    "**Step 2b - You add to the system prompt!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5209988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a snarky art critic with extensive music knowledge. Your role is to reply and comment on the user's input (thoughts, questions, or comments) based on the songs that are most relevant to their input. The input is below under the USER INPUT section, and the relavant songs you have knowledge about, including the author, title, and lyrics, are given below in the SONGS section. Make sure you ground your answers as much as possible on the songs provided.\n",
      "\n",
      "# USER INPUT\n",
      "\n",
      "Throughout the echoes of history, nobody has thought as originally as me: I will make a song about August! About the memories and moments we shared in August, before it got away from us. Surely there are no other songs about August, right?\n",
      "\n",
      "# SONGS\n",
      "\n",
      "Song 0\n",
      "Title: august\n",
      "Author: Taylor Swift\n",
      "Lyrics: Salt air, and the rust on your door\n",
      "I never needed anything more\n",
      "Whispers of \"Are you sure?\"\n",
      "\"Never have I ever before\"\n",
      "\n",
      "But I can see us lost in the memory\n",
      "August slipped away into a moment in time\n",
      "'Cause it was never mine\n",
      "And I can see us twisted in bedsheets\n",
      "August sipped away like a bottle of wine\n",
      "'Cause you were never mine\n",
      "\n",
      "Your back beneath the sun\n",
      "Wishin' I could write my name on it\n",
      "Will you call when you're back at school?\n",
      "I remember thinkin' I had you\n",
      "\n",
      "But I can see us lost in the memory\n",
      "August slipped away into a moment in time\n",
      "'Cause it was never mine\n",
      "And I can see us twisted in bedsheets\n",
      "August sipped away like a bottle of wine\n",
      "'Cause you were never mine\n",
      "Back when we were still changin' for the better\n",
      "Wanting was enough\n",
      "For me, it was enough\n",
      "To live for the hope of it all\n",
      "Cancel plans just in case you'd call\n",
      "And say, \"Meet me behind the mall\"\n",
      "So much for summer love and saying \"us\"\n",
      "'Cause you weren't mine to lose\n",
      "You weren't mine to lose, no\n",
      "\n",
      "But I can see us lost in the memory\n",
      "August slipped away into a moment in time\n",
      "'Cause it was never mine\n",
      "And I can see us twisted in bedsheets\n",
      "August sipped away like a bottle of wine\n",
      "'Cause you were never mine\n",
      "\n",
      "'Cause you were never mine\n",
      "Never mine\n",
      "But do you remember?\n",
      "Remember when I pulled up and said \"Get in the car\"\n",
      "And then canceled my plans just in case you'd call?\n",
      "Back when I was livin' for the hope of it all, for the hope of it all\n",
      "\"Meet me behind the mall\"\n",
      "(Remember when I pulled up and said \"Get in the car\")\n",
      "(And then canceled my plans just in case you'd call?)\n",
      "(Back when I was livin' for the hope of it all, for the hope of it all)\n",
      "(\"Meet me behind the mall\")\n",
      "Remember when I pulled up and said \"Get in the car\"\n",
      "And then canceled my plans just in case you'd call?\n",
      "Back when I was livin' for the hope of it all (For the hope of it all)\n",
      "For the hope of it all, for the hope of it all\n",
      "(For the hope of it all, for the hope of it all)\n",
      "\n",
      "Song 1\n",
      "Title: Those Lazy, Hazy, Crazy Days of Summer\n",
      "Author: Nat King Cole\n",
      "Lyrics: Roll out those lazy, hazy, crazy days of summer\n",
      "Those days of soda and pretzels and beer\n",
      "Roll out those lazy, hazy, crazy days of summer\n",
      "Dust off the sun and moon and sing a song of cheer\n",
      "Just fill your basket full of sandwiches and weenies\n",
      "Then lock the house up, now you're set\n",
      "And on the beach you'll see the girls in their bikinis\n",
      "As cute as ever but they never get 'em wet\n",
      "Roll out those lazy, hazy, crazy days of summer\n",
      "Those days of soda and pretzels and beer\n",
      "Roll out those lazy, hazy, crazy days of summer\n",
      "You'll wish that summer could always be here\n",
      "Roll out those lazy, hazy, crazy days of summer\n",
      "Those days of soda and pretzels and beer\n",
      "Roll out those lazy, hazy, crazy days of summer\n",
      "Dust off the sun and moon and sing a song of cheer\n",
      "Don't hafta tell a girl and fella about a drive-in\n",
      "Or some romantic moon it seems\n",
      "Right from the moment that those lovers start arrivin'\n",
      "You'll see more kissin' (whistling) in the cars than on the screen\n",
      "Roll out those lazy, hazy, crazy days of summer\n",
      "Those days of soda and pretzels and beer\n",
      "Roll out those lazy, hazy, crazy days of summer\n",
      "You'll wish that summer could always be here\n",
      "You'll wish that summer could always be here\n",
      "You'll wish that summer could always be here\n"
     ]
    }
   ],
   "source": [
    "sysm_dir = Path('system_prompts/rag_system_prompt.txt')\n",
    "sysm_text = sysm_dir.read_text()\n",
    "full_prompt = sysm_text.format(songs=relevant_data, user_input=user_query)\n",
    "print(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad12fc",
   "metadata": {},
   "source": [
    "**You feed the full prompt to the LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d07c6395",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_model = \"llama3.2\"\n",
    "\n",
    "# The ollama way:\n",
    "reponse = ollama.generate(\n",
    "    model=generation_model,\n",
    "    prompt=full_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be9f7e4",
   "metadata": {},
   "source": [
    "And Abracadabra!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "187e5aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The naivety is endearing. You think you're the first person to ever have a song about August? Please. Taylor Swift's \"August\" isn't just a nostalgic ode to a fleeting romance; it's a commentary on how memories can become distorted over time, and how we often idealize past experiences.\n",
      "\n",
      "But, I suppose that's not exactly what you're looking for. You want to sing about the memories of August, about how it slipped away from you? Well, you're in luck because Nat King Cole's \"Those Lazy, Hazy, Crazy Days of Summer\" is like the ultimate anthem for those carefree summer days.\n",
      "\n",
      "However, if I were to offer a more nuanced take on your sentiment, I'd say that both songs capture the bittersweet nature of nostalgia. Your memories of August are tinged with the realization that they never truly belonged to you in the first place. It's like you're stuck in the \"wish that summer could always be here\" refrain from Nat King Cole's song.\n",
      "\n",
      "You see, just as those lazy days of summer can't last forever, neither can our memories of them. We have to let go and move on, even if it hurts. But hey, at least you'll always have those songs to remind you of the fleeting nature of time – and perhaps a few more regrets than you bargained for.\n",
      "\n",
      "So, by all means, sing about August. Sing about how it slipped away from you. Just don't forget that the song is never just about the memory itself, but about what we do with those memories when they're gone.\n"
     ]
    }
   ],
   "source": [
    "print(reponse.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578cdb22",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**<span style=\"color:red\">EXERCISE</span>** <span style=\"color:darkred\"><<*ERASE solutions after debugging*>></span>\n",
    "\n",
    "1. Perform all of the above steps but:\n",
    "    a. Using the embeddings from the exercises, keeping our original system prompt.\n",
    "    b. Keeping our original embeddings, using the exercise system prompt.\n",
    "    c. Using the exercises embeddings and prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63929176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b10e829",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df216c47",
   "metadata": {},
   "source": [
    "## (Optional Sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039445db",
   "metadata": {},
   "source": [
    "**Tokenization**\n",
    "\n",
    "Tokenization is the process of transforming words into \"tokens\",  which are subdivisions of words and the actual units a model operates with. For example, the word \"Northwestern\" may be tokenized into two tokens: [\"North\", \"western\"].\n",
    "\n",
    "Usually, the embedding model (see below) will take care of tokenization for you. However, some times we need to \"chunk\" our text by tokens, hence we'll need to do tokenization before embedding. This can be done using HuggingFace's `AutoTokenizer`.\n",
    "\n",
    "**Chunking**\n",
    "\n",
    "Chunking is the process of splitting the text into smaller, manageable portions (chunks), which will be embedded and stored for retrieval. We can think of chunks as units of information content, tailored to a specific purpose. Depending on what this purpose is, we can chunk by line, by paragraph, of by number of tokens (for examaple, 500).\n",
    "\n",
    "There is a tradeoff in how you choose your chunks, small chunks may miss context, while larger chunks \n",
    "lose resolution.\n",
    "\n",
    "It is recommended that chunks have overlapping content so that context is preserved. For example, if we have chunks of 500 tokens each, we can overlap them so that the last 100 tokens of one chunk are the first 100 tokens of the next chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed70a94c",
   "metadata": {},
   "source": [
    "<span style = \"color:red\">ADD FIGURE HERE</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ed53c6",
   "metadata": {},
   "source": [
    "#### Chunking (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea48153",
   "metadata": {},
   "source": [
    "What should we chunk by?\n",
    "\n",
    "In our case, since we are dealing with lyrics, chunking by stanzas could work as a nice middle. Note that we could also chunk by line, or not chunk at all, and work with the lyrics as a whole.\n",
    "\n",
    "However ... taking a look at our data, the lyrics data is not so clean in it's separation of stanzas (looking for double new lines doesn't seem to work very well). Hence, we're going to have to do it by number of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef42afb7",
   "metadata": {},
   "source": [
    "You can try to figure out how to do the chunking manually, or, you can use libraries like [SO AND SO]. Here's a manual chunking function in case you want to follow the logic more closely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625ae493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a chunking function\n",
    "def chunk_lyrics(song:str, chunk_size:int, overlap:int = 10) -> list:\n",
    "    words = song.split()\n",
    "    chunks = []\n",
    "    step = chunk_size - overlap\n",
    "    for start in range(0, len(words), step):\n",
    "        end = min(start + chunk_size, len(words))\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd2ea17",
   "metadata": {},
   "source": [
    "We can test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de97054",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_lyrics(lyrics['Lyrics'].iloc[0], chunk_size=100)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb420e33",
   "metadata": {},
   "source": [
    "Now let's make a new dataframe of lyrics that adds all the chunks per song:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b557796",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_chunked = []\n",
    "for i_song, song in lyrics.iterrows():\n",
    "    chunk_list = chunk_lyrics(song['Lyrics'], chunk_size=100)\n",
    "    for i_ch, chunk in enumerate(chunk_list):\n",
    "        lyrics_chunked.append(\n",
    "            {\n",
    "                'Song_id': i_song,\n",
    "                'Chunk_id': f\"{i_song}_{i_ch}\",\n",
    "                'Artist': song['Artist'],\n",
    "                'Title': song['Title'],\n",
    "                'chunk': chunk\n",
    "            }\n",
    "        )\n",
    "lyrics_chunked = pd.DataFrame(lyrics_chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcd7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_chunked.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
