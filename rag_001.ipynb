{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6ab5a09",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa0389e",
   "metadata": {},
   "source": [
    "<span style=\"text-transform: uppercase;\n",
    "        font-size: 14px;\n",
    "        letter-spacing: 1px;\n",
    "        font-family: 'Segoe UI', sans-serif;\">\n",
    "    Author\n",
    "</span><br>\n",
    "efrén cruz cortés\n",
    "<hr style=\"border: none; height: 1px; background: linear-gradient(to right, transparent 0%, #ccc 10%, transparent 100%); margin-top: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3882a9",
   "metadata": {},
   "source": [
    "**What is Retrieval augmented generation?**\n",
    "\n",
    "See slides!\n",
    "\n",
    "**What libraries will we use?**\n",
    "- Embedding step: `sentence_transformers`, other good options available.\n",
    "- Indexing step: `faiss`, it's my favorite so far.\n",
    "- Generation step: `transformers` or `ollama`.\n",
    "\n",
    "**Pre-requisites**\n",
    "- Basic python, including numpy\n",
    "- The intro slides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39ecb99",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6056b0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks like you are working locally! Make sure you create a virtual environment and install the necessary packages.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    print(\"Looks like you are working on google Colab! Let's install the necessary packages:\")\n",
    "    !pip install faiss-cpu scikit-learn umap-learn hf_xet\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Looks like you are working locally! Make sure you create a virtual environment and install the necessary packages.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5190cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Helper libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f238944",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0beed",
   "metadata": {},
   "source": [
    "We first need \"anchoring\" data. This data will be retrieved as necessay by our system to answer relevant prompts. In our case, we are going to use a dataset of song lyrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220aae5",
   "metadata": {},
   "source": [
    "Indicate the data directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b4f8979",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data/songs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d549f580",
   "metadata": {},
   "source": [
    "Load the data into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af3d058",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c073e5",
   "metadata": {},
   "source": [
    "Let's take a look at the first few songs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d507015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Title</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>cardigan</td>\n",
       "      <td>Vintage tee, brand new phone\\nHigh heels on co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>exile</td>\n",
       "      <td>I can see you standing, honey\\nWith his arms a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>Lover</td>\n",
       "      <td>We could leave the Christmas lights up 'til Ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>the 1</td>\n",
       "      <td>I'm doing good, I'm on some new shit\\nBeen say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>Look What You Made Me Do</td>\n",
       "      <td>I don't like your little games\\nDon't like you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Artist                     Title  \\\n",
       "0  Taylor Swift                  cardigan   \n",
       "1  Taylor Swift                     exile   \n",
       "2  Taylor Swift                     Lover   \n",
       "3  Taylor Swift                     the 1   \n",
       "4  Taylor Swift  Look What You Made Me Do   \n",
       "\n",
       "                                              Lyrics  \n",
       "0  Vintage tee, brand new phone\\nHigh heels on co...  \n",
       "1  I can see you standing, honey\\nWith his arms a...  \n",
       "2  We could leave the Christmas lights up 'til Ja...  \n",
       "3  I'm doing good, I'm on some new shit\\nBeen say...  \n",
       "4  I don't like your little games\\nDon't like you...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7d62a",
   "metadata": {},
   "source": [
    "Check how many songs we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b794287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(745, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a79d551",
   "metadata": {},
   "source": [
    "And let's look at the artists we have songs from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f791a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Taylor Swift', 'Billie Eilish', 'The Beatles', 'David Bowie',\n",
       "       'Billy Joel', 'Ed Sheeran', 'Eric Clapton', 'Bruce Springsteen',\n",
       "       'Vance Joy', 'Lana Del Rey', 'Bryan Adams', 'Leonard Cohen',\n",
       "       'Nat King Cole', 'twenty one pilots', 'Ray LaMontagne',\n",
       "       'Bob Dylan', 'John Denver', 'Frank Sinatra', 'Queen', 'Elton John',\n",
       "       'George Michael'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics['Artist'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9583c45",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371fa456",
   "metadata": {},
   "source": [
    "There are many elements of preprocessing. The main ones you will encounter are:\n",
    "* Removing boilerplate text, cleaning white spaces, etc.\n",
    "* Chunking\n",
    "* Tokenization\n",
    "\n",
    "I have done the first one for you. In this workshop we may not have time to go over tokenization or chunking, but I've written sections about them at the end of the notebook, under *Optional*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef6654",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5ae6c",
   "metadata": {},
   "source": [
    "This will be our first crucial step. To create embeddings, you need to have a model that is designed for the same type of data as your data! There are models for text, for images, multimodal, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560e66bd",
   "metadata": {},
   "source": [
    "**Step 1** Choose your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a33c4fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pick a popular text model\n",
    "model_name = 'all-mpnet-base-v2' \n",
    "\n",
    "# We create the SentenceTransformer based on our model. This is the function that takes texts and produces embeddings.\n",
    "emb_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891d3c6f",
   "metadata": {},
   "source": [
    "**Step 2** Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bda5030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just one line!!\n",
    "embeddings = emb_model.encode(lyrics['Lyrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ad715",
   "metadata": {},
   "source": [
    "Now let's take a second to study our embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2561d4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SentenceTransformer returns numpy arrays. Other embedding libraries may return different data types.\n",
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90c180ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(745, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The numpy array is basically an nxd matrix, where n is number of songs and d is the embedding dimension\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f587d635",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**<span style=\"color:red\">EXERCISE</span>** <span style=\"color:darkred\"><<*ERASE solutions after debugging*>></span>\n",
    "\n",
    "Different embedding models will perform differently. Indeed, some are trained for specific purpuses in mind (Q&A, semantic search, multimodality, etc.). Throughout the exercises you'll be comparing our running example model with a diffrent embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8c36ce",
   "metadata": {},
   "source": [
    "1. Create an embedding SentenceTransformer using the model `all-MiniLM-L6-v2`. For comparison, this model is smaller than our running example model ($80$ MB, compared to $500$ MB) Call it something different to what we have above, for example `exercise_emb_model`.\n",
    "\n",
    "2. Create embeddings of our lyrics for your exercise model. Call them something like `exercise_lyrics`.\n",
    "\n",
    "3. Check if the number of dimensions is the same for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d5e510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SentenceTransformer here\n",
    "exercise_emb_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62622d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings here\n",
    "exercise_emebeddings = exercise_emb_model.encode(lyrics['Lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "165d60b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(745, 384)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of dimensions\n",
    "exercise_emebeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a30988",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dff68b",
   "metadata": {},
   "source": [
    "**Normalization**\n",
    "\n",
    "Note that our vectors are normalized (euclidean norm is 1). This is not always the case, but it is very important you know if your vectors are normalized or not. We'll get back to it when we create our index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb3dcc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(1.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the squared norm of the first embedding. If normalized, it should be 1.\n",
    "np.inner(embeddings[0], embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ec758b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization is not always ensured by default, but you can set it to be so with an argument:\n",
    "embeddings = emb_model.encode(lyrics['Lyrics'], normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a02e2d",
   "metadata": {},
   "source": [
    "SentenceTransformers provides a handy `similarity` function, which computes the pairwise similarity of two sets of songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cb0a95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4468]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing two different songs\n",
    "emb_model.similarity(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7f66f0",
   "metadata": {},
   "source": [
    "The result is a *tensor*, which you can index as you would with numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "765b312c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between song 0 and song 1 is 0.447\n"
     ]
    }
   ],
   "source": [
    "cos_sim_0_1 = emb_model.similarity(embeddings[0], embeddings[1])\n",
    "print(f\"The cosine similarity between song 0 and song 1 is {cos_sim_0_1[0,0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22f0afb",
   "metadata": {},
   "source": [
    "Note the tensor is actually a $1\\times1$ matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bdda6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim_0_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a28ea21",
   "metadata": {},
   "source": [
    "The maximum possible similarity is $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bf8d52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A maximum of 1 is achieved if the vectors are the same\n",
    "emb_model.similarity(embeddings[0], embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529360aa",
   "metadata": {},
   "source": [
    "You can compare one song to multiples songs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e1ff5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.4468, 0.6080, 0.5934, 0.5363]])\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "cos_sim_0_05 = emb_model.similarity(embeddings[0], embeddings[0:5])\n",
    "print(cos_sim_0_05)\n",
    "print(cos_sim_0_05.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc63d84",
   "metadata": {},
   "source": [
    "And now we got back a $1\\times5$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08386999",
   "metadata": {},
   "source": [
    "Or you can compare multiple songs to multiple songs, in which case you get a matrix of similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc1ab8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.4468, 0.6080, 0.5934, 0.5363],\n",
      "        [0.4468, 1.0000, 0.5352, 0.5242, 0.6023],\n",
      "        [0.6080, 0.5352, 1.0000, 0.5728, 0.4673],\n",
      "        [0.5934, 0.5242, 0.5728, 1.0000, 0.5427],\n",
      "        [0.5363, 0.6023, 0.4673, 0.5427, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "cos_sim_05_05 = emb_model.similarity(embeddings[0:5], embeddings[0:5])\n",
    "print(cos_sim_05_05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91d99dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim_05_05.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee6fabb",
   "metadata": {},
   "source": [
    "And now we got a $5\\times5$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b84d3a",
   "metadata": {},
   "source": [
    "By default, sentence_transformers uses the `cosine` similarity (see slides). But you can use other distances like the euclidean or manhattan distances (see their documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9cf7b9",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**<span style=\"color:red\">EXERCISE</span>** <span style=\"color:darkred\"><<*ERASE solutions after debugging*>></span>\n",
    "\n",
    "1. Check if the Euclidean norm of some of your `exercise_embeddings` is 1 or not.\n",
    "2. Calculate the cosine similarity for the first 5 of your exercise embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bb1bb94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(1.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.inner(exercise_emebeddings[0], exercise_emebeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9527eceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.3810, 0.5573, 0.6025, 0.5036],\n",
       "        [0.3810, 1.0000, 0.4271, 0.4929, 0.4268],\n",
       "        [0.5573, 0.4271, 1.0000, 0.5127, 0.3940],\n",
       "        [0.6025, 0.4929, 0.5127, 1.0000, 0.4785],\n",
       "        [0.5036, 0.4268, 0.3940, 0.4785, 1.0000]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exercise_emb_model.similarity(exercise_emebeddings[0:5], exercise_emebeddings[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36e909",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f1f965",
   "metadata": {},
   "source": [
    "### Visualization for Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b44e354",
   "metadata": {},
   "source": [
    "**Similarity Heatmap**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2afa4d7",
   "metadata": {},
   "source": [
    "Before continuing, let's try to develop an intuition about these embeddings.\n",
    "\n",
    "First, recall emebeddings are vectors in a d-dimensional space, where d is quite large. We can't visualize them directly, but we can see how they interact with each other. Let's look at a heatmap of the similarities between different artists songs.\n",
    "\n",
    "In the image below, we show the similarities between $5$ songs from Taylor Swift and $5$ from Bob Dylan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1174879f",
   "metadata": {},
   "source": [
    "![heatmap-swift-dylan](images/taylor_dylan_heatmap.png){width=80%}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0ae5ef",
   "metadata": {},
   "source": [
    "Seems like Taylor Swift's songs tend to be similar among themselves, while Bob Dylan's song are less similar among themselves. As expected, songs from the different artists are the most disimilar, with the exception of *Like a Rolling Stone*, which is more similar to Taylor's songs than to the rest of Dylan's!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5575f379",
   "metadata": {},
   "source": [
    "**Dimensionality Reduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e185340d",
   "metadata": {},
   "source": [
    "We can also visualize embeddings by looking at them in a lower dimension. The image below shows the embeddings in $2d$. If you want to hover over each point to find out which song it represents, open the itneractive file `images/song_embeddings.html` instead.\n",
    "\n",
    "![song-embeddings](images/song_embeddings.png){width=70%}\n",
    "\n",
    "(To see how this was done you can check the `supplementary.ipynb` file. To learn more about dimensionality reduction, you can come to my machine learning with scikit-learn workshop!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f88e0",
   "metadata": {},
   "source": [
    "### Summary of Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "455fda40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.4468, 0.6080, 0.5934, 0.5363],\n",
       "        [0.4468, 1.0000, 0.5352, 0.5242, 0.6023],\n",
       "        [0.6080, 0.5352, 1.0000, 0.5728, 0.4673],\n",
       "        [0.5934, 0.5242, 0.5728, 1.0000, 0.5427],\n",
       "        [0.5363, 0.6023, 0.4673, 0.5427, 1.0000]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can create embeddings with just a few lines:\n",
    "model_name = 'all-mpnet-base-v2' \n",
    "emb_model = SentenceTransformer(model_name)\n",
    "embeddings = emb_model.encode(lyrics['Lyrics'], normalize_embeddings=True)\n",
    "\n",
    "# and you can compare them to each other to see how similar they are (semantically)\n",
    "emb_model.similarity(embeddings[0:5], embeddings[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c86653",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**<span style=\"color:red\">EXERCISE</span>** <span style=\"color:darkred\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa009473",
   "metadata": {},
   "source": [
    "1. Write some text of your choice. We will compare it to the song lyrics. You can write something silly or nonsense.\n",
    "    - Store it in a variable named something like `exercise_my_lyrics`.\n",
    "2. Using your exercise embedding model, embed your lyrics.\n",
    "3. Compute the similarity to all other lyrics, and find the song closest to what you got. Which one is it?\n",
    "4. Repeat $2$ and $3$ above. Did the models deem the same song as closest to yours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160bf4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3584694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will help you with finding the closest song.\n",
    "    # I'm assuming you stored similarities in exercise_sims_to_my_song\n",
    "closest_song_index = list(exercise_sims_to_my_song[0]).index(max(exercise_sims_to_my_song[0]))\n",
    "print(lyrics.iloc[closest_song_index]['Lyrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb085500",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a9ec79",
   "metadata": {},
   "source": [
    "## Create Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f36a19f",
   "metadata": {},
   "source": [
    "OK, now that we have this collection of high-dimensional embeddings, we can create our index!\n",
    "\n",
    "As a reminder, an *index* is a data structure which efficiently allows us to find the most similar vectors in our collection to one reference point (usually, a user's query). See slides if you need a refresher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b130eb",
   "metadata": {},
   "source": [
    "Creating a `faiss` index takes one line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ad13b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# We need the dimension of our embeddings\n",
    "d_emb = len(embeddings[0])\n",
    "\n",
    "# Create a faiss index\n",
    "faiss_index = faiss.IndexFlatIP(d_emb) # <-- creating the d-dimensional index (empty for now)\n",
    "print(faiss_index.is_trained)\n",
    "print(faiss_index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5ffe09",
   "metadata": {},
   "source": [
    "We will talk about the meaning of `FaltIP` later. For now, just know that **only because our embeddings are normalized** this index works with the cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce28e2e",
   "metadata": {},
   "source": [
    "### Add embeddings to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2043ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dd68b982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "745\n"
     ]
    }
   ],
   "source": [
    "print(faiss_index.is_trained)\n",
    "print(faiss_index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7540a62",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**<span style=\"color:red\">EXERCISE</span>** <span style=\"color:darkred\"><<*ERASE solutions after debugging*>></span>\n",
    "\n",
    "1. Create a new `exercise_index` and add your exercise emebeddings to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc82497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c764916",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4141e39",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40920151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding info:\n",
      "\n",
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "# Make a query and embed it, don't forget to normalize!\n",
    "query_1 = \"Life is good and I will survive. I am happy that things turned out this way\"\n",
    "qemb_1 = emb_model.encode(query_1, normalize_embeddings=True)\n",
    "\n",
    "# Let's remind ourselves how the embedding is returned:\n",
    "print(f\"Query embedding info:\\n\\nType: {type(qemb_1)}\\nShape: {qemb_1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08177b",
   "metadata": {},
   "source": [
    "In theory, we can ask `faiss` to find the closest songs to it with just one line of code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9a6959",
   "metadata": {},
   "source": [
    "```python\n",
    "faiss_index.search(qemb_1, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e3db20",
   "metadata": {},
   "source": [
    "However, the above will produce an error because `faiss` expects a $(n,d)$ numpy array, but when we encode only one query we get a $(d,)$ numpy array back (afterall, `faiss` and `sentence_transformers` were developed by different people).\n",
    "\n",
    "Therefore, we need to reshape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "771d9098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding info:\n",
      "\n",
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (1, 768)\n"
     ]
    }
   ],
   "source": [
    "qemb_1 = qemb_1.reshape((1, qemb_1.shape[0])) # reshape into (1,d)\n",
    "\n",
    "print(f\"Query embedding info:\\n\\nType: {type(qemb_1)}\\nShape: {qemb_1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cb6bee",
   "metadata": {},
   "source": [
    "Now we can search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2100ec60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.42986214]], dtype=float32), array([[294]]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faiss_index.search(qemb_1, 1) # the 1 indicates how many close neighbors to find."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c087539f",
   "metadata": {},
   "source": [
    "- The first element $[[.43]]$ is the cosine similarity between the first song and its closest neighbor.\n",
    "- The second element $[[294]]$ is the index (song number) of such neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c2fc1e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity to the closest neighbor is: 0.43\n",
      "\n",
      "And the neighbor is:\n",
      "Artist                                         Eric Clapton\n",
      "Title                                       I’ll Be Alright\n",
      "Lyrics    I'll be alright\\nI'll be alright\\nI'll be alri...\n",
      "Name: 294, dtype: object\n"
     ]
    }
   ],
   "source": [
    "neighbor_sim, neighbor_idx = faiss_index.search(qemb_1, 1)\n",
    "\n",
    "print(f\"The cosine similarity to the closest neighbor is: {neighbor_sim[0,0]:.2f}\\n\")\n",
    "print(f\"And the neighbor is:\\n{lyrics.iloc[294]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e5a5a4",
   "metadata": {},
   "source": [
    "Pay special attention to the indexing of the results. If you see double brackets `[[]]` think of it as a matrix, so access it either as `[i][j]` or `[i,j]`. Even if we get one result, we'll be given, as output, a $1x1$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bc9c43",
   "metadata": {},
   "source": [
    "If we wanted to find the k closest neighors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b5cae602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.42986214, 0.41438007, 0.3375955 ]], dtype=float32),\n",
       " array([[294, 608, 566]]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 3\n",
    "faiss_index.search(qemb_1, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca178193",
   "metadata": {},
   "source": [
    "### Searching multiple queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b21d0",
   "metadata": {},
   "source": [
    "You can search for multiple queries at the same time, however, some `faiss` versions are buggy when that happens. To avoid any complications, we'll just have to search individually for each query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c387c52",
   "metadata": {},
   "source": [
    "First, let's make a helper function that does the embedding and reshaping in one step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1a86d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a helper function that automatically normalized and reshapes embeddings for us:\n",
    "def embed_reshape(query):\n",
    "    qemb = emb_model.encode(query, normalize_embeddings=True)\n",
    "    qemb = qemb.reshape((1, qemb.shape[0]))\n",
    "    return qemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6825088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"Life is good and I will survive. I am happy that things turned out this way\"\n",
    "query_2 = \"Why did you leave me? I am so sad. The world is so cruel.\"\n",
    "queries = [query_1, query_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "208de1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the queries, remember to normalize\n",
    "qembs = [embed_reshape(q) for q in queries]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba313d4",
   "metadata": {},
   "source": [
    "Let's find the 4 closest songs to each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0201bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "D_matched = []\n",
    "I_matched = []\n",
    "# using a for loop to avoid faiss buggy behavior\n",
    "for qe in qembs:\n",
    "    dists_q_matched, idxs_q_matched = faiss_index.search(qe, k)\n",
    "    D_matched.append(dists_q_matched)\n",
    "    I_matched.append(idxs_q_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5d3b4bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.42986214, 0.41438007, 0.3375955 , 0.33043662]], dtype=float32),\n",
       " array([[0.44099832, 0.43135837, 0.41771904, 0.41135132]], dtype=float32)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ee49604b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[294, 608, 566, 621]]), array([[ 76, 682,  82, 657]])]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3df30abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched song to your query 'Life is good and I will survive. I am happy that things turned out this way':\n",
      "\n",
      "Artist: Eric Clapton\n",
      "Title: I’ll Be Alright\n",
      "Lyrics:I'll be alright\n",
      "I'll be alright\n",
      "I'll be alright someday\n",
      "If in my heart\n",
      "I do not give\n",
      "Then I'll be al\n",
      "\n",
      "Artist: John Denver\n",
      "Title: Poems, Prayers, & Promises\n",
      "Lyrics:I've been lately thinking\n",
      "About my life's time\n",
      "All the things I've done\n",
      "And how it's been\n",
      "And I can'\n",
      "\n",
      "Artist: Ray LaMontagne\n",
      "Title: Part of the Light\n",
      "Lyrics:Why so many people always runnin' 'round\n",
      "Looking for a happiness that can't be found?\n",
      "I don't know\n",
      "I\n",
      "\n",
      "Artist: John Denver\n",
      "Title: Matthew\n",
      "Lyrics:I had an Uncle name of Matthew\n",
      "He was his father's only boy\n",
      "Born just south of Colby, Kansas\n",
      "He was \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's look at matched songs for the first query:\n",
    "print(f\"Matched song to your query \\'{query_1}\\':\\n\")\n",
    "for i in I_matched[0][0]:\n",
    "    artist = lyrics['Artist'].iloc[i]\n",
    "    title = lyrics['Title'].iloc[i]\n",
    "    song_lyrics = lyrics['Lyrics'].iloc[i]\n",
    "    print(f\"Artist: {artist}\\nTitle: {title}\\nLyrics:{song_lyrics[:100]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd1557c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched song to your query 'Why did you leave me? I am so sad. The world is so cruel.':\n",
      "\n",
      "Artist: Billie Eilish\n",
      "Title: ​bitches broken hearts\n",
      "Lyrics:You can pretend you don't miss me (Me)\n",
      "You can pretend you don't care\n",
      "All you wanna do is kiss me (M\n",
      "\n",
      "Artist: Queen\n",
      "Title: Too Much Love Will Kill You\n",
      "Lyrics:I'm just the pieces\n",
      "Of the man I used to be\n",
      "Too many bitter tears\n",
      "Are raining down on me\n",
      "I'm far awa\n",
      "\n",
      "Artist: Billie Eilish\n",
      "Title: ​goodbye\n",
      "Lyrics:Please, please\n",
      "Don't leave﻿ me\n",
      "Be\n",
      "\n",
      "It's not true\n",
      "Take me to the rooftop\n",
      "Told you not to worry\n",
      "What d\n",
      "\n",
      "Artist: Queen\n",
      "Title: Love of My Life\n",
      "Lyrics:Love of my life, you've hurt me\n",
      "You've broken my heart\n",
      "And now you leave me\n",
      "\n",
      "Love of my life, can't \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's look at matched songs for the second query:\n",
    "print(f\"Matched song to your query \\'{query_2}\\':\\n\")\n",
    "for i in I_matched[1][0]:\n",
    "    artist = lyrics['Artist'].iloc[i]\n",
    "    title = lyrics['Title'].iloc[i]\n",
    "    song_lyrics = lyrics['Lyrics'].iloc[i]\n",
    "    print(f\"Artist: {artist}\\nTitle: {title}\\nLyrics:{song_lyrics[:100]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8816a497",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**<span style=\"color:red\">EXERCISE</span>** <span style=\"color:darkred\"><<*ERASE solutions after debugging*>></span>\n",
    "\n",
    "1. Search for the $4$ closest songs to the first query using your exercise index.\n",
    "2. Compare the results to what we got in our main index. Are the songs the same? Do they actually make sense?\n",
    "3. Search for the $4$ closest songs to the lyrics you wrote earlier, and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc118ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "285ecd3f",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284eaee",
   "metadata": {},
   "source": [
    "### RECAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a2ec7",
   "metadata": {},
   "source": [
    "OK, let's do a quick recap, now that we have developed an intuition, we can actually perform the above steps super quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d711599e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.42986214, 0.41438007]], dtype=float32), array([[294, 608]]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# :: Load the data ::\n",
    "data_path = Path(\"data/songs.csv\")\n",
    "lyrics = pd.read_csv(data_path)\n",
    "\n",
    "# :: Create embeddings ::\n",
    "model_name = 'all-mpnet-base-v2' \n",
    "emb_model = SentenceTransformer(model_name)\n",
    "embeddings = emb_model.encode(lyrics['Lyrics'], normalize_embeddings=True)\n",
    "\n",
    "# :: Create index ::\n",
    "d_emb = len(embeddings[0])\n",
    "faiss_index = faiss.IndexFlatIP(d_emb)\n",
    "faiss_index.add(embeddings)     # Add embeddings to index\n",
    "\n",
    "# :: Search ::\n",
    "# Make query and embed it\n",
    "query_1 = \"Life is good and I will survive. I am happy that things turned out this way\"\n",
    "qemb_1 = emb_model.encode(query_1, normalize_embeddings=True)\n",
    "# Reshape if necessary\n",
    "qemb_1 = qemb_1.reshape((1, qemb_1.shape[0]))\n",
    "# Search\n",
    "k = 2\n",
    "faiss_index.search(qemb_1, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0a092",
   "metadata": {},
   "source": [
    "And there you have it!! A full retrieval system in less than 20 lines :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b1f2cd",
   "metadata": {},
   "source": [
    "## (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc69b13b",
   "metadata": {},
   "source": [
    "### A note about indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0729ade",
   "metadata": {},
   "source": [
    "**IndexFlatIP vs IndexFlatL2**\n",
    "\n",
    "IP stands for *inner product*, while L2 stands for *L2 norm* (euclidean distance). In general, these may produce different results, so you need to choose carefully. A simple heuristic is:\n",
    "- Text data: `cosine similarity`.\n",
    "- Image data: `euclidean distance`.\n",
    "\n",
    "But, you should still do some research on the model you are using, the data type, and what you care about (direction, magnitude, etc.). More details on the slides.\n",
    "\n",
    "**Cosine Similarity, Inner Product, Normalization**\n",
    "\n",
    "If and only if your vectors are *normalized*, the cosine similarity is the same as the inner product. In our case, since our embeddings are normalized, we can use `IndexFlatIP` and it will be equivalent to using the cosine similarity, which is what we want.\n",
    "\n",
    "**Flat vs Other Indexes**\n",
    "\n",
    "Our flat index is not the most computationally efficient. It doesn't quantize vectors (see slides) and all searches are brute force (that is, it will compare a query to all vectors in the index). For our toy dataset this is fine, but for larger datasets you should use other indices. Different libraries have implementations of different indices, for example `faiss` has the hierarchical navigable small world index `IndexHNSWFlat`, which is more search efficient, but returns approximate results.\n",
    "\n",
    "We can't go over all index types in this workshop, but here's a handy table for the ones `faiss` offers:\n",
    "https://github.com/facebookresearch/faiss/wiki/Faiss-indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87071f72",
   "metadata": {},
   "source": [
    "### Save index on disc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f497f75",
   "metadata": {},
   "source": [
    "In general you will create a large index, and store it on disk for further use. This could be purely for convenience (you don't need to create a new index every time you use your RAG system) but will be vital if your index is quite large and memory becomes a limitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1dbed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to a .index file\n",
    "index_directory = 'rag_workshop.index'\n",
    "faiss.write_index(faiss_index, index_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22acdde",
   "metadata": {},
   "source": [
    "If you need to read it later on, you can use `faiss.read_index(index_directory, faiss.IO_FLAG_MMAP)`, the MMAP flag tells it not to load the full index into memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-week-rag (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
