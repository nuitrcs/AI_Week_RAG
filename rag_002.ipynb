{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ce208c3",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a1c1d8",
   "metadata": {},
   "source": [
    "<span style=\"text-transform: uppercase;\n",
    "        font-size: 14px;\n",
    "        letter-spacing: 1px;\n",
    "        font-family: 'Segoe UI', sans-serif;\">\n",
    "    Author\n",
    "</span><br>\n",
    "efrén cruz cortés\n",
    "<hr style=\"border: none; height: 1px; background: linear-gradient(to right, transparent 0%, #ccc 10%, transparent 100%); margin-top: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb27f177",
   "metadata": {},
   "source": [
    "## Section 2 -  Adding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6000d386",
   "metadata": {},
   "source": [
    "OK, we are done with the R of RAG. Time to get the A done!\n",
    "\n",
    "For this, we need a generation model and a library that helps us run such model.\n",
    "\n",
    "I recommend either using `transformers` (default) or `ollama` (only if you already have it installed locally)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c85a66",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16166a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    import subprocess\n",
    "    print(\"Looks like you are working on google Colab! Let's install the necessary packages...\\n\")\n",
    "    try:\n",
    "      subprocess.run([\"nvidia-smi\"]) # add args check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE if necessary\n",
    "      print(\"You're using Colab's GPU runtime!\\n\")\n",
    "      !pip install faiss-gpu-cu12==1.12.0 hf_xet\n",
    "      # if the above doesn't work, try faiss-gpu-cu11\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "      print(\"You're NOT using Colab's GPU runtime. Embeddings will be slow :-/\\n\")\n",
    "      !pip install faiss-cpu hf_xet\n",
    "except ModuleNotFoundError:\n",
    "    print((\"Looks like you are working locally!\"\n",
    "            \" Make sure you create a virtual environment and install the necessary packages as described in the README file.\"))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b8f024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM libraries\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "try:\n",
    "    import ollama\n",
    "    print('Looks like you have ollama working. Nice!')\n",
    "except ImportError:\n",
    "    print('ollama not available. Make sure you select transformers below')\n",
    "\n",
    "# Other packages\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2328640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up local vs github directories:\n",
    "try:\n",
    "    import google.colab\n",
    "    !wget -O rag_system_prompt.txt \"https://raw.githubusercontent.com/nuitrcs/AI_Week_RAG/refs/heads/main/system_prompts/rag_system_prompt.txt\"\n",
    "    sysm_dir = Path(\"rag_system_prompt.txt\")\n",
    "except ModuleNotFoundError:\n",
    "    sysm_dir = Path('system_prompts/rag_system_prompt.txt')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f255262a",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7546c2d6",
   "metadata": {},
   "source": [
    "The following is just to help us accommodate the use of both `transformers` and `ollama`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2954963",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_lib = input(\"Which library are you using? [transformers/ollama]\")\n",
    "\n",
    "# Choose model based on library\n",
    "if gen_lib == 'transformers':\n",
    "    generation_model = \"allenai/OLMo-2-0425-1B-Instruct\"\n",
    "    print(f\"You're using {generation_model}\")\n",
    "elif 'ollama':\n",
    "    generation_model = \"llama3.2\"\n",
    "    print(f\"You're using {generation_model}. Make sure Ollama is installed in your computer!\")\n",
    "else:\n",
    "    print(\"I don't know that library. Make sure you know what you are doing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de6a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model if necessary. This may take a few minutes\n",
    "if gen_lib == 'transformers':\n",
    "    gen_ppln = pipeline(task=\"text-generation\", model=generation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce7a28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll make a handy wrapper function to account for the possible use of both libraries:\n",
    "def generate_response(prompt, gen_model, gen_lib):\n",
    "    if gen_lib == 'transformers':\n",
    "        response = gen_ppln(prompt, max_new_tokens=200)\n",
    "        response = response[0]['generated_text']\n",
    "    elif gen_lib == 'ollama':\n",
    "        response = ollama.generate(model=gen_model, prompt=prompt)\n",
    "        response = response.response\n",
    "    else:\n",
    "        print(\"Please specify either transformers or ollama\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90debc36",
   "metadata": {},
   "source": [
    "### Generating text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb3e05a",
   "metadata": {},
   "source": [
    "Some models are better at chat behavior than others. If you're using a llama model, you can just submit your question, else, you need to add some extra instructions (a system prompt) and format the query. We'll talk about this in more detail below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d587ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gen_lib == 'ollama':\n",
    "    sample_query = \"Why is the sky blue and not pink?\"\n",
    "elif gen_lib == 'transformers':\n",
    "    sample_query = (    \n",
    "        \"System: You are a knowledgable assistant who will respond to the user's query. Once a query is answered, you stop. \\n\"\n",
    "        \"User: Why is the sky blue and not pink? \\n\"\n",
    "        \"Assistant:\"\n",
    "    )\n",
    "response = generate_response(sample_query, gen_model=generation_model, gen_lib=gen_lib)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd048cf0",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "The above response may not look as good as what you're used to with ChatGPT. That's OK, we're using a much smaller model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83daca8",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**<span style=\"color:red\">EXERCISE</span>** <span style=\"color:darkred\"></span>\n",
    "\n",
    "1. Play a little bit with your generation model. Feel free to ask either serious or silly questions. Provide specific instructions on how you want it to behave, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b6140f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc79f89b",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cb0b36",
   "metadata": {},
   "source": [
    "### Creating a system prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd06561",
   "metadata": {},
   "source": [
    "Let's create a system prompt with instructions for the LLM, this system prompt will accept some extra grounding data which we obtain by searching our index. **This is the heart of RAG**, the prompt for our generation model will be supplemented by query-relevant information from a large pool, this is done through the index search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc18f8f",
   "metadata": {},
   "source": [
    "We could write our system prompt here, but these can get quite long, and maybe used in different scripts, hence I recommend writing your prompts in text files and then just loading them. Let's head to `system_prompts/rag_system_prompt.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ee6d4",
   "metadata": {},
   "source": [
    "**(Optional) Quick Review of python Strings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bff4d6f",
   "metadata": {},
   "source": [
    "We'll be modifying the system prompt with the variable output from our LLM, so here's a quick review of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec63b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_q = \"What is the answer to the ultimate question of life, the universe, and everything?\"\n",
    "temp_a = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae82974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f-strings\n",
    "f_string = f\"Question: {temp_q}\\nAnswer: {temp_a}\"\n",
    "print(f_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc32b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .format() with format fields {}\n",
    "temp_text = \"Question: {question}\\nAnswer: {answer}\"\n",
    "temp_text = temp_text.format(question=temp_q, answer=temp_a)\n",
    "print(temp_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528b6ad6",
   "metadata": {},
   "source": [
    "**Load the system prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720ab058",
   "metadata": {},
   "outputs": [],
   "source": [
    "sysm_text = sysm_dir.read_text()\n",
    "print(sysm_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1bbf7e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**<span style=\"color:red\">EXERCISE</span>** <span style=\"color:darkred\"><<*ERASE solutions after debugging*>></span>\n",
    "\n",
    "1. Create a new text file called `exercise_sysm.txt` or something like that. Write another system prompt with any instructions your heart desires. Incorporate as format fields both the user input (which can be generic, as in my case, or can be a question, a thought, a chat exchange, etc.) and other piece of information to be filled later, like songs.\n",
    "2. If you are using Colab, you will need to upload the file, click on the folder icon on the left menu bar to do so.\n",
    "3. Load the exercise system prompt (call it `exercise_sysm`) and print it to make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6498ff03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae6d11e0",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94a571c",
   "metadata": {},
   "source": [
    "**Format the System Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6425ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_song = 'Hey Macarena, ay!'\n",
    "test_question = 'What is the most philosophical song ever?'\n",
    "full_prompt = sysm_text.format(songs=test_song, user_input=test_question)\n",
    "print(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ba5d23",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**<span style=\"color:red\">EXERCISE</span>** <span style=\"color:darkred\"></span>\n",
    "\n",
    "1. Try the above with your exercise prompt, feel free to input other text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d133c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e55f672",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba558506",
   "metadata": {},
   "source": [
    "**Generate an answer based on the full prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a3899",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate_response(prompt=full_prompt, gen_model=generation_model, gen_lib=gen_lib)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549f0bd3",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**<span style=\"color:red\">EXERCISE</span>** <span style=\"color:darkred\"></span>\n",
    "\n",
    "1. Repeat, with your exercise system message + prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26680d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "350e779b",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b6737b",
   "metadata": {},
   "source": [
    "### Connecting retrieval and generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37de36e",
   "metadata": {},
   "source": [
    "OK, we have all the elements now. Let's put it all together. Let's perform our first RAAAAG!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5b5343",
   "metadata": {},
   "source": [
    "**Part 1 - You create an index from a large database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4813a311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Load the data ::\n",
    "data_path = \"https://raw.githubusercontent.com/nuitrcs/AI_Week_RAG/refs/heads/main/data/songs.csv\"\n",
    "lyrics = pd.read_csv(data_path)\n",
    "\n",
    "# :: Create embeddings ::\n",
    "model_name = 'all-mpnet-base-v2' \n",
    "emb_model = SentenceTransformer(model_name)\n",
    "embeddings = emb_model.encode(lyrics['Lyrics'], normalize_embeddings=True)\n",
    "\n",
    "# :: Create index ::\n",
    "d_emb = len(embeddings[0])\n",
    "faiss_index = faiss.IndexFlatIP(d_emb)\n",
    "# Add embeddings to index\n",
    "faiss_index.add(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffe2a96",
   "metadata": {},
   "source": [
    "**Step 2 - You perform a query and find the most relevant vectors in your index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b6c667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Search ::\n",
    "# Make query and embed it\n",
    "user_query = \"Throughout the echoes of history, nobody has thought as originally as me: I will make a song about August! About the memories and moments we shared in August, before it got away from us. Surely there are no other songs about August, right?\"\n",
    "query_emb = emb_model.encode(user_query, normalize_embeddings=True)\n",
    "# Reshape if necessary\n",
    "query_emb = query_emb.reshape((1, query_emb.shape[0]))\n",
    "# Search\n",
    "k = 2\n",
    "D_matched, I_matched = faiss_index.search(query_emb, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88aa824",
   "metadata": {},
   "source": [
    "**Step 3a - You format the relevant songs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b50e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_data = []\n",
    "for i, song_idx in enumerate(I_matched[0]):\n",
    "    row = lyrics.iloc[song_idx]\n",
    "    song_data = f\"Song {i}\\nTitle: {row['Title']}\\nAuthor: {row['Artist']}\\nLyrics: {row['Lyrics']}\"\n",
    "    relevant_data.append(song_data)\n",
    "relevant_data = \"\\n\\n\".join(relevant_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2498f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relevant_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb322e5",
   "metadata": {},
   "source": [
    "**Step 2b - You add to the system prompt!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3c6e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sysm_text = sysm_dir.read_text()\n",
    "full_prompt = sysm_text.format(songs=relevant_data, user_input=user_query)\n",
    "print(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c0ce33",
   "metadata": {},
   "source": [
    "**You feed the full prompt to the LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851ff028",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate_response(prompt=full_prompt, gen_model=generation_model, gen_lib=gen_lib)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd4f272",
   "metadata": {},
   "source": [
    "And Abracadabra!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a89b2",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**<span style=\"color:red\">EXERCISE</span>** <span style=\"color:darkred\"></span>\n",
    "\n",
    "Perform all of the above steps but:\n",
    "1. Using the embeddings from the exercises, keeping our original system prompt.\n",
    "2. Keeping our original embeddings, using the exercise system prompt.\n",
    "3. Using the exercises embeddings and prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc85dee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2af6b826",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc3fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-workshop-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
